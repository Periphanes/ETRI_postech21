{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 사용\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSMCDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, csv_file):\n",
    "    # 일부 값중에 NaN이 있음...\n",
    "    self.dataset = pd.read_csv(csv_file, sep='\\t').dropna(axis=0) \n",
    "    # 중복제거\n",
    "    self.dataset.drop_duplicates(subset=['document'], inplace=True)\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "\n",
    "    print(self.dataset.describe())\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    row = self.dataset.iloc[idx, 1:3].values\n",
    "    text = row[0]\n",
    "    y = row[1]\n",
    "\n",
    "    inputs = self.tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "    return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id          label\n",
      "count  1.461820e+05  146182.000000\n",
      "mean   6.779186e+06       0.498283\n",
      "std    2.919223e+06       0.499999\n",
      "min    3.300000e+01       0.000000\n",
      "25%    4.814832e+06       0.000000\n",
      "50%    7.581160e+06       0.000000\n",
      "75%    9.274760e+06       1.000000\n",
      "max    1.027815e+07       1.000000\n",
      "                 id         label\n",
      "count  4.915700e+04  49157.000000\n",
      "mean   6.752945e+06      0.502695\n",
      "std    2.937158e+06      0.499998\n",
      "min    6.010000e+02      0.000000\n",
      "25%    4.777143e+06      0.000000\n",
      "50%    7.565415e+06      1.000000\n",
      "75%    9.260204e+06      1.000000\n",
      "max    1.027809e+07      1.000000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NSMCDataset(\"ratings_train.txt\")\n",
    "test_dataset = NSMCDataset(\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\").to(device)\n",
    "\n",
    "# 한번 실행해보기\n",
    "# text, attention_mask, y = train_dataset[0]\n",
    "# model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 레이어 보기\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db739b0c754b467a982060dc51b865a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 66.45229667425156 Accuracy: tensor(0.6544, device='cuda:0')\n",
      "Batch Loss: 110.9630773961544 Accuracy: tensor(0.7594, device='cuda:0')\n",
      "Batch Loss: 149.73681105673313 Accuracy: tensor(0.7902, device='cuda:0')\n",
      "Batch Loss: 183.82066109776497 Accuracy: tensor(0.8100, device='cuda:0')\n",
      "Batch Loss: 214.6703191846609 Accuracy: tensor(0.8238, device='cuda:0')\n",
      "Batch Loss: 246.52186707407236 Accuracy: tensor(0.8308, device='cuda:0')\n",
      "Batch Loss: 276.4457781165838 Accuracy: tensor(0.8380, device='cuda:0')\n",
      "Batch Loss: 305.67154709249735 Accuracy: tensor(0.8430, device='cuda:0')\n",
      "Batch Loss: 334.44482234865427 Accuracy: tensor(0.8481, device='cuda:0')\n",
      "Batch Loss: 364.91080336272717 Accuracy: tensor(0.8513, device='cuda:0')\n",
      "Batch Loss: 392.0858097374439 Accuracy: tensor(0.8551, device='cuda:0')\n",
      "Batch Loss: 417.2180559299886 Accuracy: tensor(0.8588, device='cuda:0')\n",
      "Batch Loss: 443.6650833413005 Accuracy: tensor(0.8615, device='cuda:0')\n",
      "Batch Loss: 471.8328761383891 Accuracy: tensor(0.8633, device='cuda:0')\n",
      "Batch Loss: 500.11590979248285 Accuracy: tensor(0.8651, device='cuda:0')\n",
      "Batch Loss: 525.4091100245714 Accuracy: tensor(0.8671, device='cuda:0')\n",
      "Batch Loss: 552.5319139249623 Accuracy: tensor(0.8688, device='cuda:0')\n",
      "Batch Loss: 579.2658578641713 Accuracy: tensor(0.8700, device='cuda:0')\n",
      "Batch Loss: 606.1076361127198 Accuracy: tensor(0.8714, device='cuda:0')\n",
      "Batch Loss: 634.6103566847742 Accuracy: tensor(0.8725, device='cuda:0')\n",
      "Batch Loss: 659.7576784156263 Accuracy: tensor(0.8737, device='cuda:0')\n",
      "Batch Loss: 685.6943104863167 Accuracy: tensor(0.8746, device='cuda:0')\n",
      "Batch Loss: 710.942474372685 Accuracy: tensor(0.8753, device='cuda:0')\n",
      "Batch Loss: 736.9925555009395 Accuracy: tensor(0.8758, device='cuda:0')\n",
      "Batch Loss: 764.5949244890362 Accuracy: tensor(0.8758, device='cuda:0')\n",
      "Batch Loss: 791.5761383268982 Accuracy: tensor(0.8765, device='cuda:0')\n",
      "Batch Loss: 818.8278197739273 Accuracy: tensor(0.8773, device='cuda:0')\n",
      "Batch Loss: 844.5592081863433 Accuracy: tensor(0.8780, device='cuda:0')\n",
      "Batch Loss: 868.452799314633 Accuracy: tensor(0.8789, device='cuda:0')\n",
      "Batch Loss: 895.979299550876 Accuracy: tensor(0.8793, device='cuda:0')\n",
      "Batch Loss: 920.7532218080014 Accuracy: tensor(0.8801, device='cuda:0')\n",
      "Batch Loss: 945.4719951134175 Accuracy: tensor(0.8806, device='cuda:0')\n",
      "Batch Loss: 970.327633170411 Accuracy: tensor(0.8813, device='cuda:0')\n",
      "Batch Loss: 993.5914261136204 Accuracy: tensor(0.8820, device='cuda:0')\n",
      "Batch Loss: 1019.6093802172691 Accuracy: tensor(0.8826, device='cuda:0')\n",
      "Batch Loss: 1044.7678994443268 Accuracy: tensor(0.8830, device='cuda:0')\n",
      "Batch Loss: 1070.3035492170602 Accuracy: tensor(0.8832, device='cuda:0')\n",
      "Batch Loss: 1093.1383854765445 Accuracy: tensor(0.8838, device='cuda:0')\n",
      "Batch Loss: 1116.8150559160858 Accuracy: tensor(0.8844, device='cuda:0')\n",
      "Batch Loss: 1143.0321559924632 Accuracy: tensor(0.8843, device='cuda:0')\n",
      "Batch Loss: 1164.967484941706 Accuracy: tensor(0.8850, device='cuda:0')\n",
      "Batch Loss: 1188.328198498115 Accuracy: tensor(0.8854, device='cuda:0')\n",
      "Batch Loss: 1214.6686081346124 Accuracy: tensor(0.8854, device='cuda:0')\n",
      "Batch Loss: 1239.6179447788745 Accuracy: tensor(0.8858, device='cuda:0')\n",
      "Batch Loss: 1264.164260821417 Accuracy: tensor(0.8861, device='cuda:0')\n",
      "Batch Loss: 1286.0763559062034 Accuracy: tensor(0.8866, device='cuda:0')\n",
      "Batch Loss: 1308.4600508268923 Accuracy: tensor(0.8872, device='cuda:0')\n",
      "Batch Loss: 1333.3854160439223 Accuracy: tensor(0.8873, device='cuda:0')\n",
      "Batch Loss: 1357.9181859176606 Accuracy: tensor(0.8877, device='cuda:0')\n",
      "Batch Loss: 1380.9859731011093 Accuracy: tensor(0.8881, device='cuda:0')\n",
      "Batch Loss: 1406.3979122936726 Accuracy: tensor(0.8884, device='cuda:0')\n",
      "Batch Loss: 1430.5874488130212 Accuracy: tensor(0.8886, device='cuda:0')\n",
      "Batch Loss: 1456.9128682464361 Accuracy: tensor(0.8885, device='cuda:0')\n",
      "Batch Loss: 1480.441180260852 Accuracy: tensor(0.8889, device='cuda:0')\n",
      "Batch Loss: 1501.8623267728835 Accuracy: tensor(0.8895, device='cuda:0')\n",
      "Batch Loss: 1526.8276711124927 Accuracy: tensor(0.8896, device='cuda:0')\n",
      "Batch Loss: 1550.8853158075362 Accuracy: tensor(0.8899, device='cuda:0')\n",
      "Batch Loss: 1574.0638204682618 Accuracy: tensor(0.8902, device='cuda:0')\n",
      "Batch Loss: 1598.0855565536767 Accuracy: tensor(0.8904, device='cuda:0')\n",
      "Batch Loss: 1619.4249437078834 Accuracy: tensor(0.8907, device='cuda:0')\n",
      "Batch Loss: 1642.2846938241273 Accuracy: tensor(0.8911, device='cuda:0')\n",
      "Batch Loss: 1666.8670350853354 Accuracy: tensor(0.8913, device='cuda:0')\n",
      "Batch Loss: 1690.1846012566239 Accuracy: tensor(0.8917, device='cuda:0')\n",
      "Batch Loss: 1715.4641646351665 Accuracy: tensor(0.8918, device='cuda:0')\n",
      "Batch Loss: 1739.0787940397859 Accuracy: tensor(0.8919, device='cuda:0')\n",
      "Batch Loss: 1760.8786998949945 Accuracy: tensor(0.8922, device='cuda:0')\n",
      "Batch Loss: 1781.6260151788592 Accuracy: tensor(0.8926, device='cuda:0')\n",
      "Batch Loss: 1806.5454422160983 Accuracy: tensor(0.8927, device='cuda:0')\n",
      "Batch Loss: 1829.3905261587352 Accuracy: tensor(0.8929, device='cuda:0')\n",
      "Batch Loss: 1849.3915763385594 Accuracy: tensor(0.8933, device='cuda:0')\n",
      "Batch Loss: 1869.7884177602828 Accuracy: tensor(0.8937, device='cuda:0')\n",
      "Batch Loss: 1892.9391365386546 Accuracy: tensor(0.8938, device='cuda:0')\n",
      "Batch Loss: 1915.1826098896563 Accuracy: tensor(0.8942, device='cuda:0')\n",
      "Batch Loss: 1940.545045401901 Accuracy: tensor(0.8942, device='cuda:0')\n",
      "Batch Loss: 1965.5293383542448 Accuracy: tensor(0.8943, device='cuda:0')\n",
      "Batch Loss: 1988.9161709379405 Accuracy: tensor(0.8944, device='cuda:0')\n",
      "Batch Loss: 2012.7940543163568 Accuracy: tensor(0.8946, device='cuda:0')\n",
      "Batch Loss: 2034.8953761514276 Accuracy: tensor(0.8948, device='cuda:0')\n",
      "Batch Loss: 2059.642115527764 Accuracy: tensor(0.8948, device='cuda:0')\n",
      "Batch Loss: 2083.1694098021835 Accuracy: tensor(0.8950, device='cuda:0')\n",
      "Batch Loss: 2103.3610390443355 Accuracy: tensor(0.8952, device='cuda:0')\n",
      "Batch Loss: 2126.5609153155237 Accuracy: tensor(0.8954, device='cuda:0')\n",
      "Batch Loss: 2149.9446389861405 Accuracy: tensor(0.8955, device='cuda:0')\n",
      "Batch Loss: 2171.9695200771093 Accuracy: tensor(0.8957, device='cuda:0')\n",
      "Batch Loss: 2194.938929412514 Accuracy: tensor(0.8958, device='cuda:0')\n",
      "Batch Loss: 2216.542539831251 Accuracy: tensor(0.8961, device='cuda:0')\n",
      "Batch Loss: 2239.2774737514555 Accuracy: tensor(0.8962, device='cuda:0')\n",
      "Batch Loss: 2262.3361493498087 Accuracy: tensor(0.8963, device='cuda:0')\n",
      "Batch Loss: 2284.470394473523 Accuracy: tensor(0.8965, device='cuda:0')\n",
      "Batch Loss: 2306.978257302195 Accuracy: tensor(0.8966, device='cuda:0')\n",
      "Batch Loss: 2327.4298485405743 Accuracy: tensor(0.8969, device='cuda:0')\n",
      "Train Loss: 2336.5079492665827 Accuracy: tensor(0.8969, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3bb4c519f94efc8694e58bbf788779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 21.095207128673792 Accuracy: tensor(0.9175, device='cuda:0')\n",
      "Batch Loss: 41.37992431782186 Accuracy: tensor(0.9166, device='cuda:0')\n",
      "Batch Loss: 63.18407418951392 Accuracy: tensor(0.9152, device='cuda:0')\n",
      "Batch Loss: 82.36339630745351 Accuracy: tensor(0.9177, device='cuda:0')\n",
      "Batch Loss: 101.99783559888601 Accuracy: tensor(0.9189, device='cuda:0')\n",
      "Batch Loss: 124.09837130829692 Accuracy: tensor(0.9193, device='cuda:0')\n",
      "Batch Loss: 141.88926734775305 Accuracy: tensor(0.9209, device='cuda:0')\n",
      "Batch Loss: 157.8855322767049 Accuracy: tensor(0.9227, device='cuda:0')\n",
      "Batch Loss: 175.1340978294611 Accuracy: tensor(0.9243, device='cuda:0')\n",
      "Batch Loss: 195.9900174420327 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 216.30661406554282 Accuracy: tensor(0.9230, device='cuda:0')\n",
      "Batch Loss: 235.06622898206115 Accuracy: tensor(0.9231, device='cuda:0')\n",
      "Batch Loss: 254.50153522193432 Accuracy: tensor(0.9233, device='cuda:0')\n",
      "Batch Loss: 273.4039381798357 Accuracy: tensor(0.9232, device='cuda:0')\n",
      "Batch Loss: 291.1638211514801 Accuracy: tensor(0.9237, device='cuda:0')\n",
      "Batch Loss: 309.864317310974 Accuracy: tensor(0.9241, device='cuda:0')\n",
      "Batch Loss: 329.5886328164488 Accuracy: tensor(0.9242, device='cuda:0')\n",
      "Batch Loss: 351.7098158635199 Accuracy: tensor(0.9236, device='cuda:0')\n",
      "Batch Loss: 372.55275824293494 Accuracy: tensor(0.9232, device='cuda:0')\n",
      "Batch Loss: 390.99219860322773 Accuracy: tensor(0.9236, device='cuda:0')\n",
      "Batch Loss: 410.85477678477764 Accuracy: tensor(0.9236, device='cuda:0')\n",
      "Batch Loss: 431.2706547807902 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 449.82500615529716 Accuracy: tensor(0.9233, device='cuda:0')\n",
      "Batch Loss: 467.50373029801995 Accuracy: tensor(0.9237, device='cuda:0')\n",
      "Batch Loss: 487.41341343615204 Accuracy: tensor(0.9238, device='cuda:0')\n",
      "Batch Loss: 507.59962926246226 Accuracy: tensor(0.9235, device='cuda:0')\n",
      "Batch Loss: 526.9635035172105 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 547.4019891694188 Accuracy: tensor(0.9235, device='cuda:0')\n",
      "Batch Loss: 566.2557442095131 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 586.7363938894123 Accuracy: tensor(0.9235, device='cuda:0')\n",
      "Batch Loss: 608.1146355848759 Accuracy: tensor(0.9232, device='cuda:0')\n",
      "Batch Loss: 628.0072008110583 Accuracy: tensor(0.9231, device='cuda:0')\n",
      "Batch Loss: 647.2345299022272 Accuracy: tensor(0.9232, device='cuda:0')\n",
      "Batch Loss: 667.4784329952672 Accuracy: tensor(0.9231, device='cuda:0')\n",
      "Batch Loss: 685.3921933853999 Accuracy: tensor(0.9233, device='cuda:0')\n",
      "Batch Loss: 705.1086771367118 Accuracy: tensor(0.9232, device='cuda:0')\n",
      "Batch Loss: 723.6032099556178 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 742.5826527401805 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 762.5435599479824 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 781.4282723572105 Accuracy: tensor(0.9233, device='cuda:0')\n",
      "Batch Loss: 802.2343466561288 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 819.9417087864131 Accuracy: tensor(0.9234, device='cuda:0')\n",
      "Batch Loss: 840.8249701336026 Accuracy: tensor(0.9233, device='cuda:0')\n",
      "Batch Loss: 858.6792277954519 Accuracy: tensor(0.9234, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(epochs):\n",
    "  total_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  batches = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y_batch).sum()\n",
    "    total += len(y_batch)\n",
    "\n",
    "    batches += 1\n",
    "    if batches % 100 == 0:\n",
    "      print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "  \n",
    "  losses.append(total_loss)\n",
    "  accuracies.append(correct.float() / total)\n",
    "  print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4620.391721382737], [tensor(0.7406, device='cuda:0')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79c4ca2c6a1492d9b985e6857a618df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 7.24 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m input_ids_batch, attention_masks_batch, y_batch \u001b[39min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m      7\u001b[0m   y_batch \u001b[39m=\u001b[39m y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m   y_pred \u001b[39m=\u001b[39m model(input_ids_batch\u001b[39m.\u001b[39;49mto(device), attention_mask\u001b[39m=\u001b[39;49mattention_masks_batch\u001b[39m.\u001b[39;49mto(device))[\u001b[39m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m   _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(y_pred, \u001b[39m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m   test_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39m y_batch)\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:1004\u001b[0m, in \u001b[0;36mElectraForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1004\u001b[0m discriminator_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melectra(\n\u001b[0;32m   1005\u001b[0m     input_ids,\n\u001b[0;32m   1006\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1007\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1009\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1010\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1011\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1012\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1013\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1014\u001b[0m )\n\u001b[0;32m   1016\u001b[0m sequence_output \u001b[39m=\u001b[39m discriminator_hidden_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1017\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:919\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39membeddings_project\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    917\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings_project(hidden_states)\n\u001b[1;32m--> 919\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    920\u001b[0m     hidden_states,\n\u001b[0;32m    921\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    922\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    923\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    924\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    925\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    926\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    927\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    928\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    929\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:588\u001b[0m, in \u001b[0;36mElectraEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    580\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    581\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 588\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    589\u001b[0m         hidden_states,\n\u001b[0;32m    590\u001b[0m         attention_mask,\n\u001b[0;32m    591\u001b[0m         layer_head_mask,\n\u001b[0;32m    592\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    593\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    594\u001b[0m         past_key_value,\n\u001b[0;32m    595\u001b[0m         output_attentions,\n\u001b[0;32m    596\u001b[0m     )\n\u001b[0;32m    598\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:472\u001b[0m, in \u001b[0;36mElectraLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    473\u001b[0m         hidden_states,\n\u001b[0;32m    474\u001b[0m         attention_mask,\n\u001b[0;32m    475\u001b[0m         head_mask,\n\u001b[0;32m    476\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    477\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    478\u001b[0m     )\n\u001b[0;32m    479\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    481\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:399\u001b[0m, in \u001b[0;36mElectraAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    390\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    391\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 399\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    400\u001b[0m         hidden_states,\n\u001b[0;32m    401\u001b[0m         attention_mask,\n\u001b[0;32m    402\u001b[0m         head_mask,\n\u001b[0;32m    403\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    404\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    405\u001b[0m         past_key_value,\n\u001b[0;32m    406\u001b[0m         output_attentions,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    408\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    409\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programs\\anaconda3\\envs\\ETRI\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:295\u001b[0m, in \u001b[0;36mElectraSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    292\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    294\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    298\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 7.24 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "  y_batch = y_batch.to(device)\n",
    "  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "  _, predicted = torch.max(y_pred, 1)\n",
    "  test_correct += (predicted == y_batch).sum()\n",
    "  test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
